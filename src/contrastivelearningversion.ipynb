{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abae1654",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b5719e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import wandb\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from augraphy import *\n",
    "from pytorch_lightning import LightningModule, Trainer, LightningDataModule\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023b150",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558ea922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "model_list = {\n",
    "    1 : 'vit_large_patch14_clip_224.openai_ft_in12k',\n",
    "    2 : 'vit_base_patch16_clip_224.laion2b_ft_in1k',\n",
    "    3 : 'vit_pe_core_base_patch16_224.fb', # 모델없음.\n",
    "    4 : 'resnet152',\n",
    "    6 : 'vit_small_patch16_224',\n",
    "    5 : 'vit_base_patch16_224',\n",
    "    7 : 'convnext_base.fb_in22k_ft_in1k',\n",
    "    8 : 'vit_large_patch16_224',\n",
    "    9 : 'convnextv2_huge.fcmae_ft_in1k'\t, # Out of Memory\n",
    "    10 : 'convnext_large.fb_in22k_ft_in1k', \n",
    "    11 : \"convnextv2_base.fcmae_ft_in1k\",\n",
    "    12 : 'convnext_base.fb_in22k_ft_in1k_384',\n",
    "    13 : 'vit_huge_patch14_224'\t\n",
    "}\n",
    "\n",
    "model_family = {\"resnet\" : [model_list[1],\n",
    "                            model_list[2],\n",
    "                            model_list[3],\n",
    "                            model_list[4],\n",
    "                            model_list[7],\n",
    "                            model_list[9],\n",
    "                            model_list[10],\n",
    "                            model_list[11],\n",
    "                            model_list[12],],\n",
    "                \"vit\" : [model_list[6],\n",
    "                         model_list[5],\n",
    "                         model_list[8],\n",
    "                         model_list[13]]\n",
    "                            }\n",
    "\n",
    "num_classes=17\n",
    "\n",
    "# training config\n",
    "CFS={\"MODEL\" : model_list[2],\n",
    "    \"IMG_SIZE\" : 224,\n",
    "     \"LR\" : 1e-5,\n",
    "    'EPOCHS' : 200,\n",
    "    'BATCH_SIZE' : 32,\n",
    "    \"NUM_WORKERS\" : 16,\n",
    "    \"ALPHA\" : 0.2, #0.1 ~0.7\n",
    "}\n",
    "\n",
    "# wandb logging\n",
    "wandb.finish()\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"contrastive-learning\",\n",
    "    name=f\"{CFS['MODEL']},{CFS['BATCH_SIZE']},{CFS['EPOCHS']},{CFS['LR']}\",\n",
    "    config=CFS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14553fdf",
   "metadata": {},
   "source": [
    "# Augraphy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d7c46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhoppure\u001b[0m (\u001b[33mhoppure-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250708_152845-7f6dkxv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hoppure-/contrastive-learning/runs/7f6dkxv8' target=\"_blank\">vit_base_patch16_clip_224.laion2b_ft_in1k,32,200,1e-05</a></strong> to <a href='https://wandb.ai/hoppure-/contrastive-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hoppure-/contrastive-learning' target=\"_blank\">https://wandb.ai/hoppure-/contrastive-learning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hoppure-/contrastive-learning/runs/7f6dkxv8' target=\"_blank\">https://wandb.ai/hoppure-/contrastive-learning/runs/7f6dkxv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ink_phase = [\n",
    "    InkBleed(\n",
    "        intensity_range=(0.5, 0.6),\n",
    "        kernel_size=random.choice([(5, 5), (3, 3)]),\n",
    "        severity=(0.2, 0.4),\n",
    "        p=0.1,\n",
    "    ),\n",
    "    BleedThrough(\n",
    "        intensity_range=(0.1, 0.3),\n",
    "        color_range=(32, 224),\n",
    "        ksize=(17, 17),\n",
    "        sigmaX=1,\n",
    "        alpha=random.uniform(0.1, 0.2),\n",
    "        offsets=(10, 20),\n",
    "        p=0.1,\n",
    "    ),\n",
    "],\n",
    "\n",
    "paper_phase = [\n",
    "    ColorPaper(\n",
    "        hue_range=(0, 255),\n",
    "        saturation_range=(10, 40),\n",
    "        p=0.33,\n",
    "    ),\n",
    "    OneOf(\n",
    "        [\n",
    "        DelaunayTessellation(\n",
    "            n_points_range=(500, 800),\n",
    "            n_horizontal_points_range=(500, 800),\n",
    "            n_vertical_points_range=(500, 800),\n",
    "            noise_type=\"random\",\n",
    "            color_list=\"default\",\n",
    "            color_list_alternate=\"default\",\n",
    "            ),\n",
    "        PatternGenerator(\n",
    "            imgx=random.randint(256, 512),\n",
    "            imgy=random.randint(256, 512),\n",
    "            n_rotation_range=(10, 15),\n",
    "            color=\"random\",\n",
    "            alpha_range=(0.25, 0.5),\n",
    "            ),\n",
    "        VoronoiTessellation(\n",
    "            mult_range=(50, 80),\n",
    "            seed=19829813472,\n",
    "            num_cells_range=(500, 1000),\n",
    "            noise_type=\"random\",\n",
    "            background_value=(200, 255),\n",
    "            ),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    ),\n",
    "    AugmentationSequence(\n",
    "        [\n",
    "            NoiseTexturize(\n",
    "                sigma_range=(3, 10),\n",
    "                turbulence_range=(2, 5),\n",
    "            ),\n",
    "            BrightnessTexturize(\n",
    "                texturize_range=(0.9, 0.99),\n",
    "                deviation=0.03,\n",
    "            ),\n",
    "        ],\n",
    "    ),\n",
    "]\n",
    "\n",
    "post_phase = [\n",
    "    OneOf(\n",
    "        [\n",
    "            DirtyDrum(\n",
    "                line_width_range=(1, 6),\n",
    "                line_concentration=random.uniform(0.05, 0.15),\n",
    "                direction=random.randint(0, 2),\n",
    "                noise_intensity=random.uniform(0.6, 0.95),\n",
    "                noise_value=(64, 224),\n",
    "                ksize=random.choice([(3, 3), (5, 5), (7, 7)]),\n",
    "                sigmaX=0,\n",
    "                p=0.2,\n",
    "            ),\n",
    "            DirtyRollers(\n",
    "                line_width_range=(2, 32),\n",
    "                scanline_type=0,\n",
    "            ),\n",
    "        ],\n",
    "        p=0.33,\n",
    "    ),\n",
    "    Folding(\n",
    "        fold_count=10,\n",
    "        fold_noise=0.0,\n",
    "        fold_angle_range = (-360,360),\n",
    "        gradient_width=(0.1, 0.2),\n",
    "        gradient_height=(0.01, 0.1),\n",
    "        backdrop_color = (0,0,0),\n",
    "        p=0.33\n",
    "    ),\n",
    "    SubtleNoise(\n",
    "        subtle_range=random.randint(5, 10),\n",
    "        p=0.33,\n",
    "    ),\n",
    "    Jpeg(\n",
    "        quality_range=(25, 95),\n",
    "        p=0.33,\n",
    "    ),\n",
    "    Moire(\n",
    "        moire_density = (15,20),\n",
    "        moire_blend_method = \"normal\",\n",
    "        moire_blend_alpha = 0.1,\n",
    "        p=0.33\n",
    "    ),\n",
    "    ColorShift(\n",
    "        color_shift_offset_x_range=(3, 5),\n",
    "        color_shift_offset_y_range=(3, 5),\n",
    "        color_shift_iterations=(2, 3),\n",
    "        color_shift_brightness_range=(0.9, 1.1),\n",
    "        color_shift_gaussian_kernel_range=(3, 3),\n",
    "        p=0.33\n",
    "    ),\n",
    "    Scribbles(\n",
    "        scribbles_type=\"random\",\n",
    "        scribbles_location=\"random\",\n",
    "        scribbles_size_range=(250, 600),\n",
    "        scribbles_count_range=(1, 6),\n",
    "        scribbles_thickness_range=(1, 3),\n",
    "        scribbles_brightness_change=[32, 64, 128],\n",
    "        scribbles_text=\"random\",\n",
    "        scribbles_text_font=\"random\",\n",
    "        scribbles_text_rotate_range=(0, 360),\n",
    "        scribbles_lines_stroke_count_range=(1, 6),\n",
    "        p=0.1,\n",
    "    ),\n",
    "    BadPhotoCopy(\n",
    "        noise_type=-1,\n",
    "        noise_side=\"random\",\n",
    "        noise_iteration=(1, 2),\n",
    "        noise_size=(1, 3),\n",
    "        noise_value=(128, 196),\n",
    "        noise_sparsity=(0.3, 0.6),\n",
    "        noise_concentration=(0.1, 0.6),\n",
    "        blur_noise=random.choice([True, False]),\n",
    "        blur_noise_kernel=random.choice([(3, 3), (5, 5), (7, 7)]),\n",
    "        wave_pattern=random.choice([True, False]),\n",
    "        edge_effect=random.choice([True, False]),\n",
    "        p=0.33,\n",
    "    ),\n",
    "    Faxify(\n",
    "        scale_range=(0.3, 0.6),\n",
    "        monochrome=random.choice([0, 1]),\n",
    "        monochrome_method=\"random\",\n",
    "        monochrome_arguments={},\n",
    "        halftone=random.choice([0, 1]),\n",
    "        invert=1,\n",
    "        half_kernel_size=random.choice([(1, 1), (2, 2)]),\n",
    "        angle=(0, 360),\n",
    "        sigma=(1, 3),\n",
    "        p=0.1,\n",
    "    ),\n",
    "    Geometric(\n",
    "        scale=(0.5, 1.5),\n",
    "        translation=(50, -50),\n",
    "        fliplr=1,\n",
    "        flipud=1,\n",
    "        crop=(),\n",
    "        rotate_range=(3, 5),\n",
    "        p=0.33,\n",
    "    ),\n",
    "\n",
    "]\n",
    "\n",
    "pipeline = AugraphyPipeline(ink_phase=ink_phase, paper_phase=paper_phase, post_phase=post_phase)\n",
    "\n",
    "class AugraphyTransform(ImageOnlyTransform):\n",
    "    def __init__(self, augraphy_pipeline, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.augraphy_pipeline = augraphy_pipeline\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        # NumPy → PIL 변환\n",
    "        pil_img = Image.fromarray(img)\n",
    "        # Augraphy 증강 적용\n",
    "        aug_img = self.augraphy_pipeline(pil_img)\n",
    "        # PIL → NumPy 변환\n",
    "        return np.array(aug_img)\n",
    "\n",
    "Augraphy = AugraphyTransform(augraphy_pipeline=pipeline, p=0.5)\n",
    "\n",
    "wandb_logger.experiment.config[\"Augrapy\"] = str(pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b94d4",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c4d3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFS['MODEL'] in model_family['resnet']:\n",
    "    norm_mean = [0.485, 0.456, 0.406]\n",
    "    norm_std = [0.229, 0.224, 0.225]\n",
    "else:\n",
    "    norm_mean = [0.5, 0.5, 0.5]\n",
    "    norm_std = [0.5, 0.5, 0.5]\n",
    "    \n",
    "# augmentation을 위한 transform 코드\n",
    "trn_transform = A.Compose([\n",
    "    # 0. augraphy\n",
    "    Augraphy,\n",
    "    \n",
    "    # 1. 기하학적 변환 (Geometric Transformations)\n",
    "    A.OneOf([\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
    "        A.OpticalDistortion(distort_limit=0.2, p=0.5),\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5)\n",
    "    ], p=1.0),\n",
    "    \n",
    "    # 2. 공간적 변형 (Spatial Transformations)\n",
    "    A.OneOf([\n",
    "        A.RandomCrop(height=int(CFS[\"IMG_SIZE\"]*0.9), width=int(CFS[\"IMG_SIZE\"]*0.9), p=0.7),\n",
    "        A.RandomResizedCrop(size=(CFS[\"IMG_SIZE\"], CFS[\"IMG_SIZE\"]), scale=(0.8, 1.0), p=0.3),\n",
    "        A.Transpose(p=0.3), \n",
    "        A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.2),\n",
    "    ], p=1.0),\n",
    "    \n",
    "    # 3. 색상 변환 (Color Transformations)\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "        A.CLAHE(clip_limit=4.0, p=0.2),\n",
    "    ], p=1.0),\n",
    "    \n",
    "    # 4. 노이즈 및 블러 (Noise & Blur)\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), mean=0.0, per_channel=True, p=0.4),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        A.MotionBlur(blur_limit=7, p=0.3),\n",
    "    ], p=1.0),\n",
    "    \n",
    "    # 5. 고급 증강 기법 (Advanced Augmentations)\n",
    "    A.OneOf([\n",
    "        A.CoarseDropout(max_holes=8, max_height=16, max_width=16, fill_value=0, p=0.5), # cutout\n",
    "        A.RandomSunFlare(src_radius=100, p=0.1),\n",
    "        A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, p=0.2)\n",
    "    ], p=1.0),\n",
    "    \n",
    "    # 6. 최종 전처리\n",
    "    A.Resize(CFS[\"IMG_SIZE\"], CFS['IMG_SIZE']),\n",
    "    A.Normalize(mean=norm_mean, std=norm_std),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# test image 변환을 위한 transform 코드\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(CFS[\"IMG_SIZE\"], CFS['IMG_SIZE']),\n",
    "    A.Normalize(mean=norm_mean, std=norm_std),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# WandB에 로깅\n",
    "wandb_logger.experiment.config[\"train_transform\"] = str(trn_transform)\n",
    "wandb_logger.experiment.config[\"test_transform\"] = str(tst_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e151c0",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd16be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스를 정의합니다.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None, transform1=None, transform2=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.transform1 = transform1\n",
    "        self.transform2 = transform2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        if target == 3 or 7: #\n",
    "            pass #\n",
    "        else : #\n",
    "            target = 0 #\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "            return img, target\n",
    "        elif self.transform1 and self.transform2:\n",
    "            img1 = self.transform1(image=img)['image']\n",
    "            img2 = self.transform2(image=img)['image']\n",
    "            return img1, img2, target\n",
    "        else:\n",
    "            raise ValueError(\"No valid transform provided.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6151dd88",
   "metadata": {},
   "source": [
    "# datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ec3e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, data_path, train_transform, test_transform, batch_size, num_workers):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.train_transform = train_transform\n",
    "        self.test_transform = test_transform\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = ImageDataset(\n",
    "                csv=os.path.join(self.data_path, \"train.csv\"),\n",
    "                path=os.path.join(self.data_path, \"train\"),\n",
    "                transform1=self.train_transform,\n",
    "                transform2=self.train_transform,\n",
    "            )\n",
    "            \n",
    "        if stage == \"test\" or stage == \"predict\" or stage is None:\n",
    "            self.test_dataset = ImageDataset(\n",
    "                csv=os.path.join(self.data_path, \"sample_submission.csv\"),\n",
    "                path=os.path.join(self.data_path, \"test\"),\n",
    "                transform=self.test_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return self.test_dataloader()\n",
    "    \n",
    "datamodule = DataModule(data_path='../data/',\n",
    "    train_transform=trn_transform,\n",
    "    test_transform=tst_transform,\n",
    "    batch_size=CFS['BATCH_SIZE'],\n",
    "    num_workers=CFS['NUM_WORKERS']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef61a89",
   "metadata": {},
   "source": [
    "# Constrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89a81a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConLoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, features, labels=None):\n",
    "        # labels: (batch_size)\n",
    "        device = features.device\n",
    "        batch_size = features.shape[0] // 2\n",
    "        features = features.view(batch_size, 2, -1)  # (B, n_views, feat_dim)\n",
    "        features = F.normalize(features, dim=2)\n",
    "        \n",
    "        if labels is not None:\n",
    "            labels = labels.contiguous().view(-1, 1)\n",
    "            mask = torch.eq(labels, labels.T).float().to(device)  # (B, B)\n",
    "        else:\n",
    "            mask = torch.eye(batch_size, dtype=torch.float32).to(device)\n",
    "\n",
    "        contrast_count = features.shape[1]\n",
    "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)  # (B*n_views, feat_dim)\n",
    "        anchor_feature = contrast_feature\n",
    "        anchor_count = contrast_count\n",
    "\n",
    "        # Compute logits\n",
    "        anchor_dot_contrast = torch.div(\n",
    "            torch.matmul(anchor_feature, contrast_feature.T),\n",
    "            self.temperature\n",
    "        )\n",
    "\n",
    "        # For numerical stability\n",
    "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "        logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(anchor_count, contrast_count)\n",
    "\n",
    "        # mask-out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "\n",
    "\n",
    "        # Compute log_prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True) + 1e-12)\n",
    "\n",
    "        # Compute mean of log-likelihood over positive\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-12)\n",
    "\n",
    "        # Loss\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "        # ... (SupCon 공식 구현 참고)\n",
    "        # https://github.com/HobbitLong/SupContrast/blob/master/losses.py\n",
    "        # (여기서는 간략화, 실제 구현은 위 링크 참고)\n",
    "\n",
    "def knn_accuracy(embeddings, labels, k=1):\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "    sim_matrix = torch.matmul(embeddings, embeddings.T)\n",
    "    # 자기 자신 제외\n",
    "    sim_matrix.fill_diagonal_(-float('inf'))\n",
    "    topk = sim_matrix.topk(k, dim=1).indices\n",
    "    preds = labels[topk]\n",
    "    # (N, k) 중 가장 많이 나온 라벨로 예측\n",
    "    preds = torch.mode(preds, dim=1).values\n",
    "    acc = (preds == labels).float().mean().item()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a80fe",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e1ecec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(LightningModule):\n",
    "    def __init__(self, model_name, num_classes, lr, feat_dim=128, alpha=0.1):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name=model_name, \n",
    "            pretrained=True, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "        in_features = self.model.get_classifier().in_features\n",
    "        self.model.reset_classifier(0)  # 분류 헤드 제거 (backbone만 남음)\n",
    "        self.proj_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, in_features),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features, feat_dim)\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(in_features, num_classes)  # 분류 헤드 따로 추가\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.contrastive_loss = SupConLoss()\n",
    "        self.alpha = alpha\n",
    "        self.train_embeddings = []\n",
    "        self.train_targets = []\n",
    "        self.train_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.model.forward_features(x)  # ViT 등에서는 forward_features 사용\n",
    "        z = self.proj_head(feat)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        img1, img2, labels = batch  # DataLoader가 이렇게 반환해야 함\n",
    "        imgs = torch.cat([img1, img2], dim=0)  # (2*B, C, H, W)\n",
    "        features = self(imgs)\n",
    "        \n",
    "        con_loss = self.contrastive_loss(features, labels)\n",
    "\n",
    "        # 분류용 (img1만 사용)\n",
    "        feat1 = self.model.forward_features(img1)\n",
    "        logits = self.classifier(feat1)\n",
    "        logits = logits[:, 0, :]\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, labels)\n",
    "        loss = ce_loss + self.alpha * con_loss\n",
    "\n",
    "        self.log('train_loss_step', loss, prog_bar=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb = features[:labels.size(0), 0, :]  # img1의 CLS 토큰만 추출 (B, feat_dim)\n",
    "            self.train_embeddings.append(emb.detach().cpu())\n",
    "            self.train_targets.append(labels.detach().cpu())\n",
    "            self.train_losses.append(loss.detach().cpu())\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.train_embeddings:\n",
    "            all_embeddings = torch.cat(self.train_embeddings, dim=0)\n",
    "            all_targets = torch.cat(self.train_targets, dim=0)\n",
    "            acc = knn_accuracy(all_embeddings, all_targets, k=1)\n",
    "            epoch_loss = torch.stack(self.train_losses).mean()\n",
    "            \n",
    "            self.log('train_loss', epoch_loss, prog_bar=True)\n",
    "            self.log('train_knn_acc', acc, prog_bar=True)\n",
    "            \n",
    "            self.train_embeddings.clear()\n",
    "            self.train_targets.clear()\n",
    "            self.train_losses.clear()\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        img, _ = batch  # batch 구조가 (img1, img2, label)일 때만\n",
    "        feature = self.model.forward_features(img)\n",
    "        logits = self.classifier(feature)\n",
    "        logits = logits[:, 0, :]\n",
    "        return logits.argmax(dim=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
    "        scheduler = StepLR(optimizer, step_size=45, gamma=0.5)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "lightning_model = LightningModel(CFS['MODEL'], num_classes, CFS[\"LR\"], alpha=CFS['ALPHA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2302e5b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3289fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type              | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model            | VisionTransformer | 85.8 M | train\n",
      "1 | proj_head        | Sequential        | 689 K  | train\n",
      "2 | classifier       | Linear            | 13.1 K | train\n",
      "3 | contrastive_loss | SupConLoss        | 0      | train\n",
      "---------------------------------------------------------------\n",
      "86.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.5 M    Total params\n",
      "346.006   Total estimated model params size (MB)\n",
      "270       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c90c9e7c81642898ae4c0267d5805fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=200` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train_knn_acc</td><td>▁▃▃▄▄▅▅▅▆▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██▇██████</td></tr><tr><td>train_loss</td><td>█▅▄▅▄▄▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_step</td><td>▅▂▂▄▂▂▄▃▁▁▁▁▂▁▁▁▅▁▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁█▁▁▁▁▁▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>train_knn_acc</td><td>0.92803</td></tr><tr><td>train_loss</td><td>0.87498</td></tr><tr><td>train_loss_step</td><td>0.17285</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vit_base_patch16_clip_224.laion2b_ft_in1k,32,200,1e-05</strong> at: <a href='https://wandb.ai/hoppure-/contrastive-learning/runs/3iis5638' target=\"_blank\">https://wandb.ai/hoppure-/contrastive-learning/runs/3iis5638</a><br> View project at: <a href='https://wandb.ai/hoppure-/contrastive-learning' target=\"_blank\">https://wandb.ai/hoppure-/contrastive-learning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250708_123205-3iis5638/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 트레이너 설정\n",
    "trainer = Trainer(\n",
    "    max_epochs=CFS[\"EPOCHS\"],\n",
    "    accelerator='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=\"auto\",\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "\n",
    "# 학습 실행\n",
    "trainer.fit(\n",
    "    model=lightning_model,\n",
    "    datamodule=datamodule\n",
    ")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6825da63",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae4acc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50de17dbbc6a4abbbeface18b130065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(\n",
    "    model=lightning_model, \n",
    "    datamodule=datamodule\n",
    ")\n",
    "\n",
    "# 4. 결과 처리\n",
    "all_preds = torch.cat(predictions).cpu().numpy()  # [n_samples]\n",
    "# 샘플 제출 파일 로드\n",
    "submission = pd.read_csv(os.path.join(data_path, \"sample_submission.csv\"))\n",
    "# 예측값으로 타겟 열 업데이트\n",
    "submission[\"target\"] = all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1cfaa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031aaad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37544918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de95fe8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccdbd22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c6420c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
