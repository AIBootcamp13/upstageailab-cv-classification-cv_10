{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abff719-4661-45ce-8313-144423701d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import ds\n",
    "import augs\n",
    "import labs\n",
    "\n",
    "importlib.reload(ds)\n",
    "importlib.reload(labs)\n",
    "importlib.reload(augs)\n",
    "\n",
    "from ds import *\n",
    "from labs import *\n",
    "from augs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f583b398-3085-443e-833b-f92ff513b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1016) #707"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913c0cb-01dd-460d-98fc-e5f2702c255f",
   "metadata": {},
   "source": [
    "# Swin Transfromer v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc8fb015-a4b7-46af-86e7-79eda45d2f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정보\n",
    "model_name = 'microsoft/swinv2-base-patch4-window12-192-22k'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae305cca-c5b4-4ddd-8229-44776efef5aa",
   "metadata": {},
   "source": [
    "# DEF. Dataset and DataModule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9da6e065-e32f-494b-a127-27e367606f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(image_path, processor, transform):\n",
    "    # load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # load metas\n",
    "    json_path = Path(image_path).with_suffix(\".json\")\n",
    "    meta = load_json(json_path)\n",
    "\n",
    "    augmented = transform(image=image)\n",
    "    image = augmented['image']\n",
    "    \n",
    "    return processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bca021a8-d798-4263-b819-d16446630e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4Dataset(Dataset):\n",
    "    def __init__(self, image_paths, targets, processor, transform=None):\n",
    "        self.targets = targets\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        json_path = Path(image_path).with_suffix(\".json\")\n",
    "        meta = load_json(json_path)\n",
    "\n",
    "        augmented = self.transform(image=image)\n",
    "        image = augmented['image']\n",
    "        encoding = self.processor(image, return_tensors=\"pt\")\n",
    "\n",
    "        target = int(self.targets[os.path.basename(image_path)])\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": encoding[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c64b5ecb-23a1-4b1f-8c51-4619c13de1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e83f36-eae8-4744-b69a-1b8e35542933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_paths,\n",
    "        valid_paths,\n",
    "        trial_paths,\n",
    "        target_dict,\n",
    "        processor,\n",
    "        batch_size=4,\n",
    "        num_workers=6,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_paths = train_paths\n",
    "        self.valid_paths = valid_paths\n",
    "        self.trial_paths = trial_paths\n",
    "        self.targets = target_dict\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transforms = Transforms(target_size=192)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            self.train_ds = D4Dataset(self.train_paths, \n",
    "                                      self.targets, \n",
    "                                      self.processor,\n",
    "                                      self.transforms.make(50))\n",
    "            self.valid_ds = D4Dataset(self.valid_paths, \n",
    "                                      self.targets, \n",
    "                                      self.processor,\n",
    "                                      self.transforms.make(50))\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.trial_ds = D4Dataset(self.trial_paths, self.targets, self.processor, self.transforms.make(0))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator \n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trial_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd45ba1-cf97-4b28-af13-5d6d51bf5935",
   "metadata": {},
   "source": [
    "# INIT. DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244706ef-68df-49c4-8a03-5ce376dbce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5ab31df4d94bd6b3b9140ce6a86420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_paths = grep_files(\"/data/ephemeral/home/data/train\", exts=['jpg'])\n",
    "target_dict = load_csv_targets(\"/data/ephemeral/home/data/train.csv\")\n",
    "label_path = \"/data/ephemeral/home/data/doc_classes.json\"\n",
    "label2id, id2label = make_doc_class_mapper(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b49d4de-55fa-4a15-8e97-ba29dfe440fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5833dd72-ceb6-469f-922b-cc670e47bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, trial_images = split_ds(image_paths,  train_ratio=0.90,  valid_ratio=0.10, trial_ratio=0, seed=8315)\n",
    "\n",
    "data_module = D4DataModule(\n",
    "    train_paths=train_images,\n",
    "    valid_paths=valid_images,\n",
    "    trial_paths=trial_images,\n",
    "    target_dict=target_dict,\n",
    "    processor=processor,\n",
    "    batch_size=16,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae712d2-7102-4593-99a7-42e9fbe8ce8e",
   "metadata": {},
   "source": [
    "# DEF) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dd9da8-13ec-43c3-9d91-2723454bcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swinv2ForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25441067-7c05-45b9-a27d-f1975b40820c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(pl.LightningModule):\n",
    "    def __init__(self, label2id, id2label):\n",
    "        super().__init__()\n",
    "        num_classes = len(label2id)\n",
    "        \n",
    "        config = Swinv2Config.from_pretrained(model_name)\n",
    "        config.num_labels = num_classes\n",
    "        config.label2id = label2id\n",
    "        config.id2label = id2label\n",
    "\n",
    "        self.model = Swinv2ForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.model.train()\n",
    "        self.model.config.label2id = label2id\n",
    "        self.model.config.id2label = id2label\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "            # \"top-3 accuracy\" : MulticlassAccuracy(num_classes=10, top_k=3),\n",
    "            \"roc_auc\": AUROC(task=\"multiclass\", num_classes=num_classes),\n",
    "            \"precision\": Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            \"recall\": Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            \"F1\": F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        self.train_metrics = MetricCollection(metrics, prefix=\"train_\")\n",
    "        self.valid_metrics = MetricCollection(metrics, prefix=\"valid_\")\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        return self.model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "    def feed(self, batch):\n",
    "        return self(batch[\"pixel_values\"], batch[\"labels\"])\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self.feed(batch)\n",
    "    \n",
    "        self.train_metrics.update(outputs.logits, labels)\n",
    "        \n",
    "        self.log(\"train_loss\", outputs.loss)\n",
    "        for name, metric in self.train_metrics.items():\n",
    "            self.log(name, metric.compute(), prog_bar=True)\n",
    "\n",
    "        # lr 기록\n",
    "        optimizer = self.optimizers().optimizer\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        self.log('lr', current_lr)\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self.feed(batch)\n",
    "\n",
    "        self.valid_metrics.update(outputs.logits, labels)\n",
    "        \n",
    "        self.log(\"valid_loss\", outputs.loss)\n",
    "        for name, metric in self.valid_metrics.items():\n",
    "            self.log(name, metric.compute(), prog_bar=True)\n",
    "        return outputs.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # lr 1e-5 -> 5e-6, scheduler 변경\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-6, weight_decay=1e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5)\n",
    "\n",
    "        # 1. 지연 단계: total_iters 스텝 동안 초기 학습률 유지\n",
    "        scheduler_delay = ConstantLR(optimizer, factor=1.0, total_iters=30)\n",
    "        \n",
    "        # 2. 웜업 단계: 그 다음 total_iters 스텝 동안 선형 웜업 수행\n",
    "        scheduler_warmup = LinearLR(optimizer, start_factor=0.01, total_iters=10)\n",
    "        \n",
    "        # 3. 메인 스케줄링 단계 (1+2 스텝 이후): 코사인 어닐링 적용 (T_max: 전체 스텝에서 앞선 두 단계를 제외한 스텝 수)\n",
    "        total_training_steps = 1000 # 총 학습 스텝 수\n",
    "        scheduler_main = CosineAnnealingLR(optimizer, T_max=total_training_steps - 40, eta_min=1e-6)\n",
    "        \n",
    "        # scheduler_delay -> (30 스텝에서) -> scheduler_warmup -> (40 스텝에서) -> scheduler_main\n",
    "        final_scheduler = SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[scheduler_delay, scheduler_warmup, scheduler_main],\n",
    "            milestones=[40, 50]\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": final_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"monitor\": \"valid_loss\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_metrics.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.train_metrics.compute()\n",
    "        for name, value in metrics.items():\n",
    "            self.log(name, value)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.valid_metrics.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        try:\n",
    "            metrics = self.valid_metrics.compute()\n",
    "            for k, v in metrics.items():\n",
    "                self.log(k, v)\n",
    "        except Exception as e:\n",
    "            print(f\"Metric compute error: {e}\")\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "def65111-960d-448c-a903-a803034c2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_proba_map_conv(image_paths, model, class_names=None):\n",
    "    # 17개 클래스에 맞게 수정\n",
    "    ids = list(range(17))  # [0, 1, 2, ..., 16]\n",
    "    \n",
    "    # 클래스 이름이 제공되지 않으면 기본값 사용\n",
    "    if class_names is None:\n",
    "        # 주의: 이 순서가 모델 학습 시 사용한 순서와 일치해야 합니다!\n",
    "        class_names = [\n",
    "            \"계좌번호\",\n",
    "            \"임신/출산 신청서\", \n",
    "            \"자동차 계기판\",\n",
    "            \"입/퇴원 확인서\",\n",
    "            \"진단서\",\n",
    "            \"운전면허증\",\n",
    "            \"진료/의료비 영수증\",\n",
    "            \"외래/진료/통원/치료 확인서\",\n",
    "            \"주민등록증\",\n",
    "            \"여권\",\n",
    "            \"(진료비/약제비) 납입 확인서\",\n",
    "            \"약국/영수증\",\n",
    "            \"처방전\",\n",
    "            \"이력서\",\n",
    "            \"소견서\",\n",
    "            \"자동차 등록증\",\n",
    "            \"자동차 번호판\"\n",
    "        ]\n",
    "    \n",
    "    labels = class_names\n",
    "    \n",
    "    model.eval()\n",
    "    model = model.to('cuda') \n",
    "    rows = []\n",
    "\n",
    "    # ✅ 추론 루프 - 확률 분포 계산\n",
    "    for path in tqdm(image_paths):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        inputs = processor(img, return_tensors=\"pt\")['pixel_values'].cuda()\n",
    "\n",
    "        #inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "\n",
    "        # 추론 - 확률 분포 계산\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs) \n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # 소프트맥스를 적용하여 확률 분포 계산\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            probs_np = probs.cpu().numpy().squeeze()  # (num_classes,)\n",
    "            \n",
    "            # 가장 높은 확률의 클래스 찾기\n",
    "            pred_class = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # 확률을 소수점 2자리로 반올림\n",
    "        probs_rounded = [float(f\"{p:.2f}\") for p in probs_np]\n",
    "        \n",
    "        # 예측 결과 생성\n",
    "        guess = f\"{labels[pred_class]} [{ids[pred_class]}]\"\n",
    "        \n",
    "        # 결과 딕셔너리 생성\n",
    "        item = {\n",
    "            'code': os.path.basename(path),\n",
    "            'guess': guess\n",
    "        }\n",
    "        \n",
    "        # 각 클래스별 확률 추가\n",
    "        for j in range(len(ids)):\n",
    "            label = f\"{labels[j]} [{ids[j]}]\"\n",
    "            item[label] = probs_rounded[j]\n",
    "        \n",
    "        rows.append(item)\n",
    "    \n",
    "    # 파일명 기준으로 정렬\n",
    "    rows = sorted(rows, key=lambda x: x['code'])\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cad04fe6-d3b0-4ca5-bbd1-38e4c598cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, filename):\n",
    "    \"\"\"\n",
    "    결과를 CSV 파일로 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "        results: make_proba_map_layoutlmv3의 결과 (딕셔너리 리스트)\n",
    "        filename: 저장할 파일명\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # 딕셔너리 리스트를 DataFrame으로 변환\n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # CSV로 저장\n",
    "    df.to_csv(filename, index=False, encoding='utf-8-sig')  # 한글 지원을 위해 utf-8-sig 사용\n",
    "    print(f\"결과가 {filename}에 저장되었습니다.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5e598-37cb-428a-aaf9-76de1c8fcf82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d5ae25e-2f63-465f-a174-8d9885d9445f",
   "metadata": {},
   "source": [
    "# Init Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25b4435-5830-4cb9-92f5-9cea9a00f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnodient91\u001b[0m (\u001b[33mnodient91-kernel-academy\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/doc_classification-main/chy/cnn/wandb/run-20250709_233325-8m24d0hx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nodient91-kernel-academy/docsy/runs/8m24d0hx' target=\"_blank\">class-swin-base-129-transforms-aug</a></strong> to <a href='https://wandb.ai/nodient91-kernel-academy/docsy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nodient91-kernel-academy/docsy' target=\"_blank\">https://wandb.ai/nodient91-kernel-academy/docsy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nodient91-kernel-academy/docsy/runs/8m24d0hx' target=\"_blank\">https://wandb.ai/nodient91-kernel-academy/docsy/runs/8m24d0hx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = 'class-swin-base-129-transforms-aug'\n",
    "wandb.init(project='docsy', name=exp_name)\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd8e2e-e0c4-467a-aeaf-d4421531f665",
   "metadata": {},
   "source": [
    "# RUN. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00c26c73-a116-4f24-b402-b6e7b6a52d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torchmetrics import MetricCollection, Accuracy, F1Score, Precision, Recall, AUROC\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, ConstantLR, LinearLR, CosineAnnealingLR, CosineAnnealingWarmRestarts, SequentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6f94ba-6738-49b4-816a-27ea0117a0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-base-patch4-window12-192-22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([17, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Swinv2ForImageClassification, Swinv2Config\n",
    "\n",
    "model_name = \"microsoft/swinv2-base-patch4-window12-192-22k\"\n",
    "\n",
    "# 1. 정확한 config 로드\n",
    "config = Swinv2Config.from_pretrained(model_name)\n",
    "config.num_labels = len(label2id)\n",
    "config.label2id = label2id\n",
    "config.id2label = id2label\n",
    "\n",
    "# 2. 모델 생성\n",
    "model = Swinv2ForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True  # label head 크기 안 맞는 문제 무시\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b92995c1-7962-4108-b6e8-1b44b6cacf79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-base-patch4-window12-192-22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([17, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/albumentations/core/composition.py:465: UserWarning: transforms is single transform, but a sequence is expected! Transform will be wrapped into list.\n",
      "  super().__init__(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                         | Params | Mode \n",
      "-----------------------------------------------------------------------\n",
      "0 | model         | Swinv2ForImageClassification | 86.9 M | train\n",
      "1 | train_metrics | MetricCollection             | 0      | train\n",
      "2 | valid_metrics | MetricCollection             | 0      | train\n",
      "-----------------------------------------------------------------------\n",
      "86.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "86.9 M    Total params\n",
      "347.645   Total estimated model params size (MB)\n",
      "588       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03adbbd791fd42b7a53ae7c5cf6cfe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c6f33bba9743c4a427aa6b70b2a8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:295\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    293\u001b[39m     batch = call._call_strategy_hook(trainer, \u001b[33m\"\u001b[39m\u001b[33mbatch_to_device\u001b[39m\u001b[33m\"\u001b[39m, batch, dataloader_idx=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_progress\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement_ready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m trainer._logger_connector.on_batch_start(batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loops/progress.py:155\u001b[39m, in \u001b[36m_Progress.increment_ready\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `total` and `current` instances should be of the same class\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mincrement_ready\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28mself\u001b[39m.total.ready += \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      4\u001b[39m trainer = pl.Trainer(\n\u001b[32m      5\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     precision=\u001b[33m\"\u001b[39m\u001b[33m16-mixed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# callbacks=[model_checkpoint, early_stopping]\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m model = CNN(label2id, id2label)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='valid_loss', patience=10, mode='min')\n",
    "model_checkpoint = ModelCheckpoint(monitor=\"valid_loss\", mode=\"min\", save_top_k=2)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs=50,\n",
    "    logger=wandb_logger,\n",
    "    reload_dataloaders_every_n_epochs=1, \n",
    "    callbacks=[model_checkpoint]\n",
    "    # callbacks=[model_checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "model = CNN(label2id, id2label)\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddca7464-041e-4118-89db-578b29d61840",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(f\"./{exp_name}-last_epoch.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16140809-01b5-4e27-962d-1bda72304101",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>lr</td><td>▇▆▂▁▁▅█▆▅▁█▃▂▂▃▇▅▂▁▁▂▆█▇▂▂▁▃█▄▁▁▂██▂▂▃▇▇</td></tr><tr><td>train_F1</td><td>▁▂▂▃▄▄▅▄▅▄▅▄▄▅▆▅▄▆▇▆▆▆▇▆▇▇▇▇▇▅▇▇▇▇▄▅▇█▇▅</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▄▅▃▅▄▅▄▆▅▆▅█▄▆▅▆▇▇▅▇▇▅▅▇▆▆▇█▅▇▅▇▆▇█</td></tr><tr><td>train_loss</td><td>▆█▅▆▄▄▄▂▅▂▃▆▄▃▅▃▁▁▃▄▂▂▅▂▃▃▃▂▁▃▁▄▁▃▃▁▂▂▁▃</td></tr><tr><td>train_precision</td><td>▁▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█████▇██████▇█▇▇▇██████</td></tr><tr><td>train_recall</td><td>▁▂▃▁▂▃▂▃▃▃▄▄▂▃▅▄▅▄▇▃▅▃▄▅▃▆▄▄▆▅▃▆▃▆█▄▆▄▇▆</td></tr><tr><td>train_roc_auc</td><td>▃▁▂▃▄▄▃▅▄▅▅▅▅▅▆▆▅▆▅▇▆▅▇▇▆▆▇▆▆█▆█▆██████▆</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>valid_F1</td><td>▁▃▃▃▆▆▃▅▅▄▅▆▅▅▇▇▆▇▇█▇▆█▇▇▇▆▇▇█▆█▇▆▆██▇█▇</td></tr><tr><td>valid_accuracy</td><td>▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██▇█▇████▇█▇████</td></tr><tr><td>valid_loss</td><td>█▆▅▄▆▂▁▁▂▃▄▁▂▃▂▁▂▃▄▃▄▄▅▃▃▅▅▄▄▂▃▄▆▂▇▃▄▇▆▆</td></tr><tr><td>valid_precision</td><td>▁▆▆▆▆▆▆▅▆▆▆▇▇▆▇▆▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇</td></tr><tr><td>valid_recall</td><td>▁▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇█▇█▇▆█▇▇▇▇▇█▇▇▇▇▇▇██▇██</td></tr><tr><td>valid_roc_auc</td><td>▁▇▇▇▇▇▇▇▇▇██████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>199</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>train_F1</td><td>0.87402</td></tr><tr><td>train_accuracy</td><td>0.89809</td></tr><tr><td>train_loss</td><td>0.07873</td></tr><tr><td>train_precision</td><td>0.87357</td></tr><tr><td>train_recall</td><td>0.88685</td></tr><tr><td>train_roc_auc</td><td>0.99196</td></tr><tr><td>trainer/global_step</td><td>17799</td></tr><tr><td>valid_F1</td><td>0.87402</td></tr><tr><td>valid_accuracy</td><td>0.89809</td></tr><tr><td>valid_loss</td><td>0.47037</td></tr><tr><td>valid_precision</td><td>0.87357</td></tr><tr><td>valid_recall</td><td>0.88685</td></tr><tr><td>valid_roc_auc</td><td>0.99196</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exp-swin-base-129-transforms-aug</strong> at: <a href='https://wandb.ai/nodient91-kernel-academy/docsy/runs/zicg70jr' target=\"_blank\">https://wandb.ai/nodient91-kernel-academy/docsy/runs/zicg70jr</a><br> View project at: <a href='https://wandb.ai/nodient91-kernel-academy/docsy' target=\"_blank\">https://wandb.ai/nodient91-kernel-academy/docsy</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250709_163432-zicg70jr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50594587-61f6-4514-a128-11e64257c3fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32279bee-d2f3-4599-83e7-696ec1d0dd5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8174c67f6b8a409aa36039d821181838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img_paths = grep_files(\"/data/ephemeral/home/data/test\", exts=['jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2598d2ed-a948-4209-9c9e-62f70290db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def logits_to_prob_table(logits, class_names, labels=None):\n",
    "    probs = F.softmax(logits, dim=1)  # (batch, num_classes)\n",
    "    probs_np = probs.detach().cpu().numpy()\n",
    "    batch_size, num_classes = probs_np.shape\n",
    "\n",
    "    num_classes = probs_np.shape[1]\n",
    "    if labels is not None:\n",
    "        labels_np = labels.detach().cpu().numpy() if torch.is_tensor(labels) else labels\n",
    "    else:\n",
    "        labels_np = [''] * batch_size\n",
    "\n",
    "    records = []\n",
    "    columns = ['정답'] + [f'{name}' for name in class_names]\n",
    "    for i in range(batch_size):\n",
    "        row = [labels_np[i]] + [\"{:.4f}\".format(p) for p in probs_np[i]] \n",
    "        records.append(row)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9999e3de-6a6f-4d24-904c-6688ab739f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Swinv2ForImageClassification were not initialized from the model checkpoint at microsoft/swinv2-base-patch4-window12-192-22k and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([21841, 1024]) in the checkpoint and torch.Size([17, 1024]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([21841]) in the checkpoint and torch.Size([17]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/data/ephemeral/home/doc_classification-main/chy/cnn/exp-swin-base-129-transforms-aug-last_epoch.ckpt\"\n",
    "model = CNN.load_from_checkpoint(model_path, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40d3852d-84c0-412b-aa92-15b38fcc0f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd9fba8507e4096b66962621f87e32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 [AJH]swin-proba-map-desque-test.csv에 저장되었습니다.\n",
      "총 1570개 이미지 처리 완료\n",
      "                   code                  guess  계좌번호 [0]  임신/출산 신청서 [1]  \\\n",
      "0  002f99746285dfdd.jpg           자동차 번호판 [16]       0.0            0.0   \n",
      "1  008ccd231e1fea5d.jpg  (진료비/약제비) 납입 확인서 [10]       0.0            0.0   \n",
      "2  008f5911bfda7695.jpg  (진료비/약제비) 납입 확인서 [10]       0.0            0.0   \n",
      "3  009235e4c9c07af5.jpg                진단서 [4]       0.0            0.0   \n",
      "4  00b2f44967580c74.jpg           자동차 번호판 [16]       0.0            0.0   \n",
      "\n",
      "   자동차 계기판 [2]  입/퇴원 확인서 [3]  진단서 [4]  운전면허증 [5]  진료/의료비 영수증 [6]  \\\n",
      "0          0.0           0.0      0.0        0.0             0.0   \n",
      "1          0.0           0.0      0.0        0.0             0.0   \n",
      "2          0.0           0.0      0.0        0.0             0.0   \n",
      "3          0.0           0.0      1.0        0.0             0.0   \n",
      "4          0.0           0.0      0.0        0.0             0.0   \n",
      "\n",
      "   외래/진료/통원/치료 확인서 [7]  주민등록증 [8]  여권 [9]  (진료비/약제비) 납입 확인서 [10]  약국/영수증 [11]  \\\n",
      "0                  0.0        0.0     0.0                    0.0          0.0   \n",
      "1                  0.0        0.0     0.0                    1.0          0.0   \n",
      "2                  0.0        0.0     0.0                    1.0          0.0   \n",
      "3                  0.0        0.0     0.0                    0.0          0.0   \n",
      "4                  0.0        0.0     0.0                    0.0          0.0   \n",
      "\n",
      "   처방전 [12]  이력서 [13]  소견서 [14]  자동차 등록증 [15]  자동차 번호판 [16]  \n",
      "0       0.0       0.0       0.0           0.0           1.0  \n",
      "1       0.0       0.0       0.0           0.0           0.0  \n",
      "2       0.0       0.0       0.0           0.0           0.0  \n",
      "3       0.0       0.0       0.0           0.0           0.0  \n",
      "4       0.0       0.0       0.0           0.0           1.0  \n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 추론 실행\n",
    "results = make_proba_map_conv(\n",
    "    image_paths, \n",
    "    model\n",
    ")\n",
    "\n",
    "# 2. CSV 저장\n",
    "df = save_results_to_csv(results, \"[AJH]swin-proba-map-desque-test.csv\")\n",
    "\n",
    "# 3. 결과 확인\n",
    "print(f\"총 {len(df)}개 이미지 처리 완료\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a187ec76-70ff-4345-8351-b42a7095e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2781b69f-20cf-4f9c-a514-0a3f7d0465c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56e6518ae5f4595aed9477afbab52e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = []\n",
    "for p in tqdm(test_img_paths):\n",
    "    img = cv2.imread(p)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = processor(img, return_tensors=\"pt\")['pixel_values']\n",
    "    img = img.cuda()\n",
    "    outs = model(img)\n",
    "\n",
    "    predict = torch.argmax(outs.logits, dim=1)\n",
    "    pred = (p, predict.item())\n",
    "    \n",
    "    # top3_values, top3_indices = torch.topk(outs.logits, k=3, dim=1)\n",
    "    # pred = (top3_values.cpu().numpy(), top3_indices.cpu().numpy())\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df0f055e-dfc5-4e30-a7e0-5c303bfaf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_value(csv_path, preds):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    for i, item in enumerate(preds):\n",
    "        filename = os.path.basename(item[0])\n",
    "        df.loc[df['ID'] == filename, 'target'] = item[1]\n",
    "        \n",
    "    df.to_csv(csv_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5f65c0c6-190b-4cc9-a287-1f9e4e453797",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"/data/ephemeral/home/pred3.csv\"\n",
    "write_csv_value(csv_path, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "466a7778-575d-4332-a0cb-5668742a02c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 3/3142 [00:00<02:15, 23.11it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m labels = [\u001b[33m'\u001b[39m\u001b[33m임신/출산\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m입퇴원확인서\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m진단서\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m의료비영수증\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m진료통원확인서\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m납입확인서\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m약국영수증\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m처방전\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m이력서\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m소견서\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# model, processor는 이미 학습된 상태라고 가정\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m rows = \u001b[43mmake_proba_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_img_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/py12/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mmake_proba_map\u001b[39m\u001b[34m(image_paths, model, processor, labels, ids, device)\u001b[39m\n\u001b[32m     24\u001b[39m probs = [\u001b[38;5;28mfloat\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs]\n\u001b[32m     26\u001b[39m idx = \u001b[38;5;28mint\u001b[39m(torch.argmax(torch.tensor(probs)))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m guess = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids[idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m item = {\u001b[33m'\u001b[39m\u001b[33mcode\u001b[39m\u001b[33m'\u001b[39m: os.path.basename(p), \u001b[33m'\u001b[39m\u001b[33mguess\u001b[39m\u001b[33m'\u001b[39m: guess, \u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m: ids[idx]}\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ids)):\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 정의된 클래스 정보\n",
    "ids = [1, 3, 4, 6, 7, 10, 11, 12, 13, 14]\n",
    "labels = ['임신/출산', '입퇴원확인서', '진단서', '의료비영수증', '진료통원확인서', '납입확인서', '약국영수증', '처방전', '이력서', '소견서']\n",
    "\n",
    "# model, processor는 이미 학습된 상태라고 가정\n",
    "rows = make_proba_map(test_img_paths, model=model, processor=processor, labels=labels, ids=ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0e517-6dce-4e85-84d1-2a8005df235d",
   "metadata": {},
   "source": [
    "# 체크포인트 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9f669-548b-43f9-b125-3782d8824535",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = \"/data/ephemeral/home/doc_classification-main/chy/cnn/exp-swin-large-192-transforms-aug-last_epoch.ckpt\"\n",
    "model = CNN.load_from_checkpoint(ckpt_path, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2538e48-98be-4126-afc6-e132ca9566b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model = model.to('cuda') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c4a6265-6e70-43fc-97c0-8955cc57cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(test_img_paths[0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = processor(img, return_tensors=\"pt\")['pixel_values']\n",
    "img = img.cuda()\n",
    "outs = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fad44bb9-f382-4987-8f8c-3ccd5f50ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_trans = {\n",
    "    \"account_number\":\"계좌번호\",\n",
    "    \"application_for_payment_of_pregnancy_medical_expenses\": \"임신/출산 신청서\",\n",
    "    \"car_dashboard\": \"자동차 계기판\",\n",
    "    \"confirmation_of_admission_and_discharge\": \"입퇴원 확인서\",\n",
    "    \"diagnosis\": \"진단서\",\n",
    "    \"driver_lisence\": \"운전면허증\",\n",
    "    \"medical_bill_receipts\": \"진료/의료비 영수증\",\n",
    "    \"medical_outpatient_certificate\": \"(외래)진료(통원/치료) 확인서\",\n",
    "    \"national_id_card\": \"주민등록증\",\n",
    "    \"passport\": \"여권\",\n",
    "    \"payment_confirmation\": \"(진료비/약제비) 납입 확인서\",\n",
    "    \"pharmaceutical_receipt\": \"약국/영수증\",\n",
    "    \"prescription\": \"처방전\",\n",
    "    \"resume\": \"이력서\",\n",
    "    \"statement_of_opinion\": \"소견서\",\n",
    "    \"vehicle_registration_certificate\": \"자동차 등록증\",\n",
    "    \"vehicle_registration_plate\": \"자동차\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d732c7c-a796-4b3e-ae65-e53b381df6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20db0ea2-4f79-4068-bc08-457ca9c13986",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmake_proba_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmake_proba_map\u001b[39m\u001b[34m(image_paths)\u001b[39m\n\u001b[32m      5\u001b[39m texts = []\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m image_paths:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     texts.append([\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(\u001b[43mget_words\u001b[49m(p))])\n\u001b[32m      9\u001b[39m rows = []\n\u001b[32m     10\u001b[39m preds =  predictor.predict_proba({\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m:texts})\n",
      "\u001b[31mNameError\u001b[39m: name 'get_words' is not defined"
     ]
    }
   ],
   "source": [
    "make_proba_map(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5875c57-090a-439e-b73e-9c529d86606f",
   "metadata": {},
   "source": [
    "# 체크포인트 저장하고 로드하고 로짓으로 TOP3 혹은 전체 표 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96059aab-9d61-4726-b0e8-a489e5a7eac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>정답</th>\n",
       "      <th>계좌번호</th>\n",
       "      <th>임신/출산 신청서</th>\n",
       "      <th>자동차 계기판</th>\n",
       "      <th>입퇴원 확인서</th>\n",
       "      <th>진단서</th>\n",
       "      <th>운전면허증</th>\n",
       "      <th>진료/의료비 영수증</th>\n",
       "      <th>(외래)진료(통원/치료) 확인서</th>\n",
       "      <th>주민등록증</th>\n",
       "      <th>여권</th>\n",
       "      <th>(진료비/약제비) 납입 확인서</th>\n",
       "      <th>약국/영수증</th>\n",
       "      <th>처방전</th>\n",
       "      <th>이력서</th>\n",
       "      <th>소견서</th>\n",
       "      <th>자동차 등록증</th>\n",
       "      <th>자동차</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3659</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  정답    계좌번호 임신/출산 신청서 자동차 계기판 입퇴원 확인서     진단서   운전면허증 진료/의료비 영수증  \\\n",
       "0     0.0000    0.0000  0.0000  0.6340  0.0000  0.0000     0.0000   \n",
       "\n",
       "  (외래)진료(통원/치료) 확인서   주민등록증      여권 (진료비/약제비) 납입 확인서  약국/영수증     처방전     이력서  \\\n",
       "0            0.3659  0.0000  0.0000           0.0000  0.0000  0.0000  0.0000   \n",
       "\n",
       "      소견서 자동차 등록증     자동차  \n",
       "0  0.0001  0.0000  0.0000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = [label_trans[k] for k in label2id.keys()]\n",
    "logits_to_prob_table(outs.logits, class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76840b-67b2-47d0-815c-b5013986f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(test_img_paths[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7476525-0daf-4124-84fc-bffbc1900276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "py12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
