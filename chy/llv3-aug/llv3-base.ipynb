{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2abff719-4661-45ce-8313-144423701d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import labs\n",
    "\n",
    "importlib.reload(labs)\n",
    "from labs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbb40d-3091-4032-bdb4-0b9e2e4a16e1",
   "metadata": {},
   "source": [
    "# INIT. Processor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab262aa5-4743-4ac4-bcf8-8f3021fb1975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1fd9eb7-9002-48a8-8930-b7d335a8dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57ddd5c-99d2-4d7c-bc71-34054854e439",
   "metadata": {},
   "source": [
    "# DEF. Augmentation\n",
    "### 주의\n",
    "- augmentaion은 LayoutLM processor 전 단계에서 수행되고 processor는 이미지의 pixel이 양수인 이미지를 기대함\n",
    "- aug 단계에서 이미지 normalize를 해버리면 Processor에서 받는 픽셀 intensity가 음수가 될 수 있어 에러가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573bac92-1f56-4c5e-8ebe-73889303aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_augmentation(aug_prob=0.8, target_size=224):\n",
    "    augs = [\n",
    "        # 아핀 변환: 이동, 스케일, 회전\n",
    "        A.Affine(\n",
    "            translate_percent=0.2,  # 이동 범위 (이미지 크기 대비 %)\n",
    "            scale=(0.8, 1.2),       # 스케일 범위 (배율)\n",
    "            rotate=(-85, 85),       # 회전 범위 (각도)\n",
    "            shear=(-10, 10),        # 전단 변형 추가 \n",
    "            p=0.7\n",
    "        ),\n",
    "        \n",
    "        # 가우시안 노이즈\n",
    "        A.GaussNoise(std_range=(0.1, 0.2), p=0.3),\n",
    "        # 가우시안 블러\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),                                     \n",
    "        \n",
    "        # dropout\n",
    "        A.CoarseDropout(num_holes_range=(3, 6), hole_height_range=(10, 20), hole_width_range=(10, 20), \n",
    "                        fill=\"random_uniform\", p=0.2),\n",
    "        # crop\n",
    "        A.RandomCrop(height=target_size, width=target_size),\n",
    "        \n",
    "        # 흑백 전환\n",
    "        A.ToGray(p=0.15),                                                             \n",
    "        # 색상 전환\n",
    "        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3), \n",
    "        # 밝기/대비\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),  \n",
    "        \n",
    "        # 좌우 플립\n",
    "        A.HorizontalFlip(p=0.5),                                                      \n",
    "        # 상하 플립\n",
    "        A.VerticalFlip(p=0.2),                                                        \n",
    "    ]\n",
    "    \n",
    "    return A.Compose([\n",
    "        A.OneOf([ \n",
    "            A.RandomOrder(augs, p=aug_prob),\n",
    "            A.NoOp(p=1-aug_prob)\n",
    "        ], p=1.0),\n",
    "        # A.Normalize(normalization=\"image\", p=0.9),\n",
    "        ToTensorV2()\n",
    "    ], \n",
    "    bbox_params=A.BboxParams(\n",
    "        format='pascal_voc',     # pascal_voc가 x0, y0, x1, y1 포맷 \n",
    "        label_fields=['words'],  # bbox와 대응되어 양항을 받는 속성들\n",
    "        min_area=0,                    \n",
    "        min_visibility=0.0,            \n",
    "        check_each_transform=True,     \n",
    "        clip=True                      \n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae305cca-c5b4-4ddd-8229-44776efef5aa",
   "metadata": {},
   "source": [
    "# DEF. Dataset and DataModule \n",
    "### 주의\n",
    "- 또한, bbox의 좌표가 layoutlm v3 (0,1000) 스케일이 아닌 픽셀 스케일을 원함\n",
    "  - 이에 따라, norm_box 변환 시점을 augment 뒤로 미뤄야 함\n",
    "\n",
    "```\n",
    "return_tensors (str, optional, defaults to \"pt\") — The type of Tensor to return. Allowable values are “np”, “pt” and “tf”.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da6e065-e32f-494b-a127-27e367606f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_example(image_path, processor, transform=None):\n",
    "    # load image\n",
    "    if not transform:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = ImageOps.exif_transpose(image)  # correct orientation\n",
    "    else:\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # load metas\n",
    "    json_path = Path(image_path).with_suffix(\".json\")\n",
    "    meta = load_json(json_path)\n",
    "\n",
    "    # words and boxes\n",
    "    words, boxes = get_words_and_boxes(image, meta, use_norm=transform is None)\n",
    "\n",
    "    if transform is not None:\n",
    "        augmented = transform(image=image, bboxes=boxes, words=words)\n",
    "        image = augmented['image']\n",
    "        words = augmented['words']\n",
    "        boxes = augmented['bboxes']\n",
    "        h, w = image.shape[:2]\n",
    "        boxes = [to_norm_box_with_size(b, h, w) for b in boxes] \n",
    "\n",
    "    encoding = processor(\n",
    "        images=image,\n",
    "        text=words,\n",
    "        boxes=boxes,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca021a8-d798-4263-b819-d16446630e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4Dataset(Dataset):\n",
    "    def __init__(self, image_paths, targets, processor, transform=None):\n",
    "        self.targets = targets\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        encoding = prepare_example(image_path, self.processor, self.transform)\n",
    "        target = int(self.targets[os.path.basename(image_path)])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"bbox\": encoding[\"bbox\"].squeeze(0),\n",
    "            \"pixel_values\": encoding[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e83f36-eae8-4744-b69a-1b8e35542933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D4DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_paths,\n",
    "        valid_paths,\n",
    "        trial_paths,\n",
    "        target_dict,\n",
    "        processor,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_paths = train_paths\n",
    "        self.valid_paths = valid_paths\n",
    "        self.trial_paths = trial_paths\n",
    "        self.targets = target_dict\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        transform = make_augmentation(aug_prob=0.8, target_size=224)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            self.train_ds = D4Dataset(self.train_paths, \n",
    "                                      self.targets, \n",
    "                                      self.processor,\n",
    "                                      make_augmentation(aug_prob=0.8, target_size=224))\n",
    "            self.valid_ds = D4Dataset(self.valid_paths, \n",
    "                                      self.targets, \n",
    "                                      self.processor,\n",
    "                                      make_augmentation(aug_prob=0.8, target_size=224))\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.trial_ds = D4Dataset(self.trial_paths, self.targets, self.processor)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valid_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator \n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trial_ds,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            collate_fn=default_data_collator \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd45ba1-cf97-4b28-af13-5d6d51bf5935",
   "metadata": {},
   "source": [
    "# INIT. DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "244706ef-68df-49c4-8a03-5ce376dbce28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e6ae79ca4648449955f19eec6436ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_paths = grep_files(\"/data/ephemeral/home/dataset/dtc/train\", exts=['jpg'])\n",
    "target_dict = load_csv_targets(\"/data/ephemeral/home/dataset/dtc/train.csv\")\n",
    "label_path = \"/data/ephemeral/home/dataset/dtc/doc_classes.json\"\n",
    "label2id, id2label = make_doc_class_mapper(label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5833dd72-ceb6-469f-922b-cc670e47bc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, trial_images = split_ds(image_paths,  train_ratio=0.6,  valid_ratio=0.4, test_ratio=0)\n",
    "\n",
    "data_module = D4DataModule(\n",
    "    train_paths=train_images,\n",
    "    valid_paths=valid_images,\n",
    "    trial_paths=trial_images,\n",
    "    target_dict=target_dict,\n",
    "    processor=processor,\n",
    "    batch_size=16,\n",
    "    num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b638a78d-411d-4fa0-8e40-cc7b65a49660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s0 = data_module.train_ds[0]\n",
    "# print(s0.keys())\n",
    "# print(s0['input_ids'].shape, s0['attention_mask'].shape, s0['bbox'].shape, s0['pixel_values'].shape, s0['labels'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae712d2-7102-4593-99a7-42e9fbe8ce8e",
   "metadata": {},
   "source": [
    "# DEF) Model\n",
    "- ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight'] 크기 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dd9da8-13ec-43c3-9d91-2723454bcd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForSequenceClassification as LyLmv3, LayoutLMv3Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59cf9574-b7cc-4ba7-85f3-beac25fae36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lym(pl.LightningModule):\n",
    "    def __init__(self, label2id, id2label):\n",
    "        super().__init__()\n",
    "        num_classes = len(label2id)\n",
    "        self.model = LyLmv3.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=num_classes)\n",
    "        self.model.config.label2id = label2id\n",
    "        self.model.config.id2label = id2label\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": Accuracy(task=\"multiclass\", num_classes=num_classes),\n",
    "            # \"top-3 accuracy\" : MulticlassAccuracy(num_classes=10, top_k=3),\n",
    "            \"roc_auc\": AUROC(task=\"multiclass\", num_classes=num_classes),\n",
    "            \"precision\": Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            \"recall\": Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "            \"F1\": F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        self.train_metrics = MetricCollection(metrics, prefix=\"train_\")\n",
    "        self.valid_metrics = MetricCollection(metrics, prefix=\"valid_\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None):\n",
    "        return self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            bbox=bbox,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "    def feed(self, batch):\n",
    "        return self(\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"bbox\"],\n",
    "            batch[\"pixel_values\"],\n",
    "            batch[\"labels\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self.feed(batch)\n",
    "    \n",
    "        self.train_metrics.update(outputs.logits, labels)\n",
    "        \n",
    "        self.log(\"train_loss\", outputs.loss)\n",
    "        for name, metric in self.train_metrics.items():\n",
    "            self.log(name, metric.compute(), prog_bar=True)\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"labels\"]\n",
    "        outputs = self.feed(batch)\n",
    "\n",
    "        self.valid_metrics.update(outputs.logits, labels)\n",
    "        \n",
    "        self.log(\"valid_loss\", outputs.loss)\n",
    "        for name, metric in self.valid_metrics.items():\n",
    "            self.log(name, metric.compute(), prog_bar=True)\n",
    "        return outputs.loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        self.train_metrics.reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metrics = self.train_metrics.compute()\n",
    "        for name, value in metrics.items():\n",
    "            self.log(name, value)\n",
    "\n",
    "    def on_validation_epoch_start(self):\n",
    "        self.valid_metrics.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        try:\n",
    "            metrics = self.valid_metrics.compute()\n",
    "            for k, v in metrics.items():\n",
    "                self.log(k, v)\n",
    "        except Exception as e:\n",
    "            print(f\"Metric compute error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ae25e-2f63-465f-a174-8d9885d9445f",
   "metadata": {},
   "source": [
    "# Init Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c25b4435-5830-4cb9-92f5-9cea9a00f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcatchy\u001b[0m (\u001b[33mcat2oon\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/ephemeral/home/mlc/code/docsy/llv3c/wandb/run-20250702_002521-ckdsku5z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cat2oon/docsy/runs/ckdsku5z' target=\"_blank\">exp-llv3-aug-test</a></strong> to <a href='https://wandb.ai/cat2oon/docsy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cat2oon/docsy' target=\"_blank\">https://wandb.ai/cat2oon/docsy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cat2oon/docsy/runs/ckdsku5z' target=\"_blank\">https://wandb.ai/cat2oon/docsy/runs/ckdsku5z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = 'exp-llv3-aug-test'\n",
    "wandb.init(project='docsy', name=exp_name)\n",
    "wandb_logger = WandbLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd8e2e-e0c4-467a-aeaf-d4421531f665",
   "metadata": {},
   "source": [
    "# RUN. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92995c1-7962-4108-b6e8-1b44b6cacf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of LayoutLMv3ForSequenceClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type                                | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | model         | LayoutLMv3ForSequenceClassification | 125 M  | eval \n",
      "1 | train_metrics | MetricCollection                    | 0      | train\n",
      "2 | valid_metrics | MetricCollection                    | 0      | train\n",
      "------------------------------------------------------------------------------\n",
      "125 M     Trainable params\n",
      "0         Non-trainable params\n",
      "125 M     Total params\n",
      "503.723   Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "243       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d16bde1cb34a1ca56c3730018ff2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/transformers/modeling_utils.py:1072: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/data/ephemeral/home/.pyenv/versions/py12/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66441f15c8034d6a982abc16471eb92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa436166aa3b4a34a1941351ecde4a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2447cf793d284100b553bdd5bc5208db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='valid_loss', patience=5, mode='min')\n",
    "model_checkpoint = ModelCheckpoint(monitor=\"valid_loss\", mode=\"min\", save_top_k=3)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs=100,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[model_checkpoint, early_stopping]\n",
    ")\n",
    "\n",
    "model = Lym(label2id, id2label)\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16140809-01b5-4e27-962d-1bda72304101",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd0c69-4eea-4182-8557-7ac7c45b5116",
   "metadata": {},
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb94c42-f282-4e42-9eb1-4d39106e7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_tensor(model, name):\n",
    "    if not model:\n",
    "        model = model.load_from_checkpoint(\"checkpoint.ckpt\")\n",
    "    state_dict = model.state_dict()\n",
    "    save_file(state_dict, f\"{name}.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18543c29-c0ae-443f-83b7-c3d063f1938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_tensor(model, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597cb91-6dc0-4c35-ac3f-4368125c693e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "py12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
